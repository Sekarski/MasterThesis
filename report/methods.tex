\chapter{Methods}
%LM,KNN,Tree,ANN,Garch - qu'est-ce que c'est et comment ça marche

\section{Time Series Methods}
\subsection{ARMA}
An \acrfull{arma}(p,q) model is defined by:
$$
Y_t = \sum_{j=1}^p \phi_j Y_{t-j} + \sum_{j=1}^q \theta_j \epsilon_{t-j} + \epsilon_t
$$
Where the $\phi_j$'s and $\theta_j$'s are constants with $\phi_p , \theta_q \neq 0$. Usually $\epsilon_t \stackrel{iid}{\sim} \mathcal{N}(0,1)$ but could be any distribution with zero mean and unit variance.
\subsection{GARCH}
A \acrfull{garch}(m,r) model is defined by:
$$
Y_t = \sigma_t \epsilon_t
$$
$$
\sigma_t^2 = \alpha_0 + \sum_{j=1}^m \alpha_j Y_{t-j}^2 + \sum_{j=1}^r \beta_j \sigma_{t-j}^2
\quad, \epsilon_t \stackrel{iid}{\sim} F(0,1)
$$
where $F$ is any distribution with zero mean and unit variance. Often it is either Normal or Student t.
\acrshort{garch} models are generally the \acrlong{ts} models used for financial data, but more generally it is used any time the variance is clearly not constant. A \acrshort{garch} model can also be combined with an \acrshort{arma} model, to create and \acrshort{arma}-\acrshort{garch} model. As we will see later on, this will not be necessary here so we will not go further into these models.

\section{Machine Learning Methods}
We'll go over the various methods we will be using, giving a brief description and overview of the pros and cons of each and when it is appropriate to use them.
We will suppose that there are $n$ observations and $m$ features.


\subsection{Linear Models}
The idea is to use a linear combination of features to determine the response. The linear constants are the parameters that need to be chosen to give the best approximation of the response space using the feature space. Geometrically this means fitting a hyperplane to the training set, by minimizing some function (loss function).
Formally:
$$
Y_j = \sum_{i=1}^m \beta_{i} X_{ji} \quad, j=1,\dots,n
$$

\subsubsection{Advantages}
\acrlong{lm} are some of the most common and simple models out there, but they have proven to be reliable and can be made quite complex with feature engineering. There plenty of theory and proofs about these methods. This all makes these methods and obvious first choice of methods to consider.
%insert "toute est linéaire" quote
\subsubsection{Problems}


\subsection{K Nearest Neighbours}
The intuition behind \acrshort{knn} methods is simple. The idea is that observations (points in the feature space) which are "close" will have responses that are also "close". Then a good approximation would be given by averaging the responses of the K observations "closest" to the new observation.
In this model, we need to chose the definition of "close" as well as chose the value of K.
Usually, the distance used to define close, is the euclidean distance and that is the one we used. As for K this parameter needs to be tuned. The larger it is the smoother the result is and the lower the more jagged it is. For optimal tuning the parameter should be as small as possible, without over-fitting.
\subsubsection{Advantages}
It's very intuitive and very simple to implement. 
It's non parametric ? <- no prior knowledge or guess about the prior distribution
\subsubsection{Problems}
It suffers from curse of dimensionality. When the dimension of the feature space gets large, the space becomes sparsely populated, and the K closest observations, might actually be very far from each other, and thus not reflect well the new observation.
As a workaround for this problem, there is a variant of \acrshort{knn} which instead of using the K closest observations, uses all the observations which fall in a certain radius of the new observation point. Thus the K parameter becomes R, the radius of the hyperball.

\subsection{Trees}
To understand more sophisticated tree methods, we first need to understand how a simple tree works for regression.
A simple binary tree, or a Decision Tree, work by splitting the feature space recursively into simple strats. Once a tree has been grown, the predictor $\hat{y}_i$ of $x_i$ is the average of all the $y_i$ in the strat that $x_i$ falls into. The prediction function is therefore piecewise constant.
Formally:
$$
T(x) = \sum_{m=1}^M c_m I(x \in A_m)
$$
Where there are $M$ strats denoted by $A_m$ and $c_m$ are the averaging constants.

\subsubsection{Advantages}
Non parametric
The tree is inexpensive to use as it has a complexity that is logarithmic
The models are simple to understand
\subsubsection{Disadvantages}
High variance
Over-fitting is easy
Unstable. Change the data a little and the tree could look completely different
Optimal tree is NP-hard so we use greedy alogrithms
Predictions tree are not smooth.


The variations we shall consider are
- Random Forests
- Extremely randomized trees

Random Forests work by taking many Decision Trees constructed by taking random samples of the training set, with replacement, and then averaging the trees. At each split a random number of variables are chosen, thus ensuring that if a variable is predominant, it won't always be chosen first in each tree of the forest, thus decreasing the correlation of the trees amongst themselves and in turn increasing the reduction in variance.
This will reduce the variance, with a cost of raising the bias a little.

Extremely randomized trees are random forests, but in each tree, the split is determined by a random threshold, decreasing even more the correlation of trees. 
Again this leads to less variance but a slightly higher bias

\subsection{Artificial Neural Networks}