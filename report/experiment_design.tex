\chapter{Experiment Design}
\label{chap:experiment_design}
%need better name for this chapter. Can't remember what this is called when we're not actually doing experiments.
Let $(\mathcal{X},\mathcal{Y})$ denote our data, where $\mathcal{X}$ is the input data (we will use the terms "features", "predictors" and "variables" as well throughout this paper) and $\mathcal{Y}$ is the output data (we will also use "response").
We shall separate all our data into 2 sets: the training set $(\mathcal{X}_{train},\mathcal{Y}_{train})$ and the test set $(\mathcal{X}_{test},\mathcal{Y}_{test})$
The split date was chosen to be January 4th 2021 at 10:00. This was a rather arbitrary choice, to leave about one months worth of data for testing. Let us denote the split date as $T$
We will train our models on the training set, and then predict responses for observations in the test set. That is we will predict a set $\mathcal{Y}_{predict} = F(\mathcal{X}_{test})$ where $F()$ is the model's prediction function.


\section{Metrics}
%R^2
%MSE
%others ???

To compare the results and rank them, we shall use several metrics to asses goodness of fit and predictive power. The main two we shall consider are \acrfull{mse} and  \Gls{R2}.

Let us denote $\mathcal{Y}_{test} = \{ y_i : i=1, \dots, m \}$ an arbitrary test set of size $m$. And let us denote $\mathcal{Y}_{predict} = \{ \hat{y}_i : i=1, \dots. m\}$ such that $\hat{y}_i$ is the predicted value of $y_i$

\subsection{\acrshort{mse}}
The \acrlong{mse} is defined as $MSE(\mathcal{Y}_{predict},\mathcal{Y}_{test}) = \frac{1}{m}\sum_{i=1}^m (y_i - \hat{y}_i)^2$
\subsection{\Gls{R2}}
\Gls{R2} denotes the coefficient of determination.

The one used by our \Gls{python} package, is defined as \footnote{ \url{https://scikit-learn.org/stable/modules/model_evaluation.html\#r2-score } (June 24, 2021)}

$R^2(\mathcal{Y}_{predict},\mathcal{Y}_{test}) = 1 - \frac{\sum_{i=1}^m(y_i - \hat{y}_i)^2}{\sum_{i=1}^m(y_i - \bar{y}_i)^2}$

Where $\bar{y} = \frac{1}{m} \sum_{i=1}^m y_i$. %(source (25.05.2021): https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score)

The best score is 1.0, or a model that always predicts exactly the right value. A constant model that always predicts the expected value of $y$ without taking into account the input, would be 0.0. The score can be arbitrarily negative, as the top term can be arbitrarily large (i.e. our predictions can be arbitrarily bad). So for our methods to be better than just using the mean of the observations, we would like to have an \Gls{R2} larger than zero.

\section{set-up}
%ML univariate response: using one value of each feature for predict one value of response
%ML multivariate resposne: using one month of features are one point in the feature hyperplane to predict one week of responses as one point in the response hyperplane
Let $P_{t}$ denote the stock price at timestep $t$.

\subsection{\acrlong{ts}}
For the \acrlong{ts} set-up, the response variable is $\mathcal{Y}=\{ P_{t} \}$ and the features are $\mathcal{X} = \{ (P_{t-1}, \dots, P_{t-p}) \}$ and where $p$ is a parameter that will be determined through the \acrlong{ts} analysis.

\subsection{\acrshort{ml} univariate response}
In this set-up we consider the response variable to be univariate $\mathcal{Y}=\{P_t\}$ and features to be $ \mathcal{X} = \{ (t,GT1_t,GT2_t,TW1_t,TW2_t)  \}$. Where the variables are from left to right: time, the google trend for 'tesla', the google trend for 'musk', Elon Musks tweet likes, Elon Musks tweet retweets, all at time $t$

We shall then train the models on the training set, and then predict one hour ahead for all the observations in the test set, giving us one week ahead worth of predictions, and compare with the values of the test set.

\subsection{\acrshort{ml} multivariate response}
\sloppy In this set-up we consider the response variable to be multivariate 
$\mathcal{Y}=\{ (P_t,\dots,P_{t+r}) \}$
We shall consider the response to be one week ahead worth of data, or $r=35$ steps. To predict this multivariate response, we shall augment the feature space dimensions, by not only considering $X_{j_t}$'s as predictors, but also $X_{j_{t-1}},X_{j_{t-2}},...,X_{j_{t-p}}$ as predictors (Where the $X_j$ are the same variables from the univariate case, as well as the price at times $t-1, \dots , t-p$). We shall consider one month of past data (or $p=140$ time steps) as part of one observation. A sort of hybrid \acrlong{ts} - \acrlong{ml} approach.

In the first set-up, the forecast of observation $y_{T+1}$ is generally good but the forecast of each observation after that gets more and more vague, faster and faster. To the point where after a number of time steps, the forecasts hold no more value and become effectively useless. The idea in the second setup, is to put a certain number of future observations into the response variable, so that when forecasting, the "first observation" that is forecast (so the one that is usually the best) now contains contains multiple steps ahead. Of course this comes at a price; using the same feature space, but with more response variables, will cause the fit and predictions to be worse. So we need to augment the feature space as well, by considering multiple steps behind as part of one observation point. This is actually what the Time Series methods actually already do. They use a combination of previous values to predict the next values.

We shall then train this on the training set, and predict one step ahead (which effectively will be one week ahead) and compare the values with those of the test set.
